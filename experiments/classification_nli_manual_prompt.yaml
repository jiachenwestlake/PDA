dataset:
  name: mnlimulti
  path: datasets/TextClassification/mnli_for_dg
  domains:
    - telephone
    - travel
    - fiction
    - government
    - slate
  target_domain: slate
  adv: False
  adv_class: True
  kl_div: False

plm:
  model_name: roberta
  model_path: roberta-base
  optimize:
    freeze_para: False
    lr: 0.00001 #0.00001
    weight_decay: 0.01
    scheduler:
      type: 
      num_warmup_steps: 500

dataloader:
  max_seq_length: 128

train:
  batch_size: 10
  num_epochs: 30

test:
  batch_size: 30

dev:
  batch_size: 30



template: soft_manual_template
verbalizer: manual_verbalizer

soft_manual_template:
  choice: 1
  file_path: scripts/TextClassification/mnli/soft_manual_template.txt
  optimize:
    lr: 0.00003 #0.003
    weight_decay: 0.0
    scheduler:
      num_warmup_steps: 0


#manual_template:
#  choice: 0
#  file_path: scripts/TextClassification/amazon/manual_template.txt


#one2one_verbalizer:
#  choice: 0
#  file_path: scripts/TextClassification/amazon/manual_verbalizer.txt

manual_verbalizer:
  choice: 0
  file_path: scripts/TextClassification/mnli/multiwords_verbalizer.jsonl
  
environment:
  num_gpus: 1
  cuda_visible_devices:
    - 0
    - 1
  local_rank: 0

learning_setting: full

#few_shot:
#  parent_config: learning_setting
#  few_shot_sampling: sampling_from_train
#
#sampling_from_train:
#  parent_config: few_shot_sampling
#  num_examples_per_label: 10
#  also_sample_dev: True
#  num_examples_per_label_dev: 10
#  seed: 123

task: classification
classification:
  parent_config: task
  metric:  # the first one will be the main  to determine checkpoint.
    - accuracy  # whether the higher metric value is better.
  loss_function: cross_entropy ## the loss function for classification
